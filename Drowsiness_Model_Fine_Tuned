import os
import math
import numpy as np
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

# Dataset Paths
DATASET_DIR = "/kaggle/input/eyes-open-or-closed/dataset"
TRAIN_DIR = os.path.join(DATASET_DIR, "train")
VAL_DIR = os.path.join(DATASET_DIR, "test")

# Hyperparameters
BATCH_SIZE = 32
IMG_SIZE = (224, 224)
EPOCHS = 20
LEARNING_RATE = 0.001

# Data Augmentation
train_datagen = ImageDataGenerator(
    rescale=1.0 / 255.0,
    rotation_range=15,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)

# Load Data (Binary Classification)
train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="binary"  # Changed to binary
)

val_generator = val_datagen.flow_from_directory(
    VAL_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="binary"  # Changed to binary
)

# Load Pretrained ResNet50
base_model = ResNet50(
    weights="/kaggle/input/resnet-weights/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5",
    include_top=False,
    input_shape=(224, 224, 3)
)

# Freeze the base layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom classification head (Simplified)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation="relu")(x)  # Reduced from 1024 to 512
x = Dropout(0.4)(x)  # Adjusted dropout rate
output = Dense(1, activation="sigmoid")(x)  # Binary classification

model = Model(inputs=base_model.input, outputs=output)

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE),
    loss="binary_crossentropy",  # Changed loss
    metrics=["accuracy"]
)

# Callbacks (Monitor validation accuracy)
checkpoint = ModelCheckpoint("drowsiness_model.keras",  # Changed extension
                            save_best_only=True, 
                            monitor="val_accuracy", 
                            mode="max")

early_stopping = EarlyStopping(
    monitor="val_accuracy",  # Changed to monitor accuracy
    patience=5,
    mode="max",
    restore_best_weights=True
)

# Calculate steps with ceiling
steps_per_epoch = math.ceil(train_generator.samples / BATCH_SIZE)
validation_steps = math.ceil(val_generator.samples / BATCH_SIZE)

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,  # Updated steps
    validation_data=val_generator,
    validation_steps=validation_steps,  # Updated steps
    epochs=EPOCHS,
    callbacks=[checkpoint, early_stopping]
)

# Save the initial trained model
model.save("drowsiness_model_initial.keras") 

# Fine-tune the model (Last 20 layers)
for layer in base_model.layers[:-20]:
    layer.trainable = False
for layer in base_model.layers[-20:]:
    layer.trainable = True

# Lower learning rate for fine-tuning
model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE / 10),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

# Fine-tuning phase
fine_tune_history = model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_generator,
    validation_steps=validation_steps,
    epochs=EPOCHS,
    callbacks=[checkpoint, early_stopping]
)

# Save the fine-tuned model
model.save("drowsiness_model_finetuned.h5")
